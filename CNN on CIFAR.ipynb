{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DenseNet - cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "colab": {}
      },
      "source": [
        "# import keras\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "# from keras.layers import Concatenate\n",
        "# from keras.optimizers import Adam\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNHw6luQg3gc",
        "colab": {}
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsO_yGxcg5D8",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 40\n",
        "num_filter = 12\n",
        "compression = 0.5\n",
        "compression_1 = 2\n",
        "compression_2 = 4\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0c079e9b-d039-40bc-d526-523cb6c88883"
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAF4KRT-f95K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=X_train.astype(\"float32\")  \n",
        "X_test=X_test.astype(\"float32\")\n",
        "mean=np.mean(X_train)\n",
        "std=np.std(X_train)\n",
        "X_test=(X_test-mean)/std\n",
        "X_train=(X_train-mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lAk_Mw_5-rn",
        "colab_type": "code",
        "outputId": "d3441edf-0236-4df7-a93c-83b69d48feaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVkpgHsc5-rp",
        "colab_type": "code",
        "outputId": "3d445b0d-c33f-4c79-9e7b-92f10f1824a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJPM2dPxgfTy",
        "colab_type": "text"
      },
      "source": [
        "Image Augmentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "934PoDAPgYDc",
        "colab_type": "code",
        "outputId": "e52fdbc2-d88a-41da-fb3b-4ccfd14a9d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(width_shift_range=0.3, height_shift_range=0.3, horizontal_flip=True,rotation_range=15)\n",
        "datagen.fit(X_train)\n",
        "img_train = datagen.flow(X_train, y_train, batch_size=64)\n",
        "steps = int(X_train.shape[0] / 64)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHKMtL0cgoSa",
        "colab_type": "text"
      },
      "source": [
        "Kernel of size 3x3 running for 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ee-sge5Kg7vr",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    global compression_1\n",
        "    global compression_2\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        #if dropout_rate>0:\n",
        "            #Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Block\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    #if dropout_rate>0:\n",
        "         #Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    #flat = layers.Flatten()(AvgPooling)\n",
        "    First = layers.Conv2D(int(10), (2,2), use_bias=False ,padding='same')(AvgPooling)\n",
        "    Maxpool = layers.GlobalMaxPooling2D()(First)\n",
        "    output = layers.Activation(\"softmax\")(Maxpool)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anPCpQWhhGb7",
        "colab": {}
      },
      "source": [
        "num_filter = 12\n",
        "dropout_rate = 0.2\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5Wksy8z5-rw",
        "colab_type": "code",
        "outputId": "e25eb443-0d55-46e2-c1ba-9d3160409632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "#https://arxiv.org/pdf/1608.06993.pdf\n",
        "from IPython.display import IFrame, YouTubeVideo\n",
        "YouTubeVideo(id='-W6y8xnd--U', width=600)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/-W6y8xnd--U\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f37ce2df8d0>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kFh7pdxhNtT",
        "outputId": "e3bf09e7-a0b8-4455-f01f-730359740481",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8765
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 12)   324         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 12)   48          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 12)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 6)    648         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 18)   0           conv2d[0][0]                     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 18)   72          concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 18)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 6)    972         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 24)   0           concatenate[0][0]                \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 24)   96          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 24)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 6)    1296        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 30)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 30)   120         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 30)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 6)    1620        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 36)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 36)   144         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 36)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 6)    1944        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 42)   0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 42)   168         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 42)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 6)    2268        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 48)   0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 6)    2592        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 54)   0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 54)   216         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 54)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 6)    2916        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 60)   0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 60)   240         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 60)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 6)    3240        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 66)   0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 66)   264         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 66)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 6)    3564        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 72)   0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 72)   288         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 72)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 6)    3888        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 78)   0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 78)   312         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 78)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 6)    4212        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 84)   0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 84)   336         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 84)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 6)    504         activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 6)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 6)    24          average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 6)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 6)    324         activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 12)   0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 12)   48          concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 12)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 6)    648         activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 18)   0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 18)   72          concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 18)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 6)    972         activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 24)   0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 24)   96          concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 24)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 6)    1296        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 30)   0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 30)   120         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 30)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 6)    1620        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 36)   0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 36)   144         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 36)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 6)    1944        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 42)   0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 42)   168         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 42)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 6)    2268        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 48)   0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 48)   192         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 6)    2592        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 54)   0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 54)   216         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 54)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 6)    2916        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 60)   0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 60)   240         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 60)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 6)    3240        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 66)   0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 66)   264         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 66)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 6)    3564        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 72)   0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 72)   288         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 72)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 6)    3888        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 78)   0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 78)   312         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 78)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 6)    468         activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 6)      0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 6)      24          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 6)      0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 6)      324         activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 8, 8, 12)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 12)     48          concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 12)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 6)      648         activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 18)     0           concatenate_24[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 18)     72          concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 18)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 6)      972         activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 24)     0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 24)     96          concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 24)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 6)      1296        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 30)     0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 30)     120         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 30)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 6)      1620        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 36)     0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 36)     144         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 36)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 6)      1944        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 42)     0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 42)     168         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 42)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 6)      2268        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 48)     0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 48)     192         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 48)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 6)      2592        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 54)     0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 54)     216         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 54)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 6)      2916        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 60)     0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 60)     240         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 60)     0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 6)      3240        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 66)     0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 66)     264         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 66)     0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 6)      3564        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 72)     0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 72)     288         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 72)     0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 6)      3888        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 78)     0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 78)     312         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 78)     0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 6)      468         activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 6)      0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 4, 6)      24          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 4, 4, 6)      0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 4, 4, 6)      324         activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 4, 4, 12)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 12)     48          concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 12)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 6)      648         activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 18)     0           concatenate_36[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 18)     72          concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 18)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 6)      972         activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 24)     0           concatenate_37[0][0]             \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 24)     96          concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 24)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 6)      1296        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 30)     0           concatenate_38[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 30)     120         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 30)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 6)      1620        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 36)     0           concatenate_39[0][0]             \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 36)     144         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 36)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 6)      1944        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 42)     0           concatenate_40[0][0]             \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 42)     168         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 42)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 6)      2268        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 48)     0           concatenate_41[0][0]             \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 48)     192         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 48)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 6)      2592        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 54)     0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 54)     216         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 54)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 6)      2916        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 60)     0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 60)     240         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 60)     0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 6)      3240        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 66)     0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 66)     264         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 66)     0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 6)      3564        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 72)     0           concatenate_45[0][0]             \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 72)     288         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 72)     0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 6)      3888        activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 78)     0           concatenate_46[0][0]             \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 78)     312         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 78)     0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 78)     0           activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 2, 2, 10)     3120        average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 10)           0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 10)           0           global_max_pooling2d[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 118,908\n",
            "Trainable params: 114,384\n",
            "Non-trainable params: 4,524\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOmoG1cAwvz0",
        "colab_type": "code",
        "outputId": "27d6c5a6-5e54-481a-af87-dff004f163e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#https://www.tensorflow.org/tensorboard/scalars_and_keras\n",
        "filepath=\"weights.best.hdf5\"\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "callback_1 = tf.keras.callbacks.ModelCheckpoint(filepath=filepath ,\n",
        "                                save_weights_only=True,\n",
        "                                monitor=\"val_accuracy\",\n",
        "                                mode=\"max\",\n",
        "                                save_best_only=True,\n",
        "                                verbose=1)\n",
        "tensorboard_1 = TensorBoard(log_dir='graph_one', batch_size=16,update_freq='epoch')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v2.py:92: UserWarning: The TensorBoard callback `batch_size` argument (for histogram computation) is deprecated with TensorFlow 2.0. It will be ignored.\n",
            "  warnings.warn('The TensorBoard callback `batch_size` argument '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4XOsW3ahSkL",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "crhGk7kEhXAz",
        "outputId": "cbe2fc18-0ee5-4e6e-b6e8-6f77c3090295",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3728
        }
      },
      "source": [
        "history = model.fit_generator(img_train,\n",
        "                    steps_per_epoch=steps,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=callback_1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-16fbaf398460>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.7687 - accuracy: 0.3304\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.39300, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 125s 159ms/step - loss: 1.7687 - accuracy: 0.3304 - val_loss: 1.7819 - val_accuracy: 0.3930\n",
            "Epoch 2/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.4507 - accuracy: 0.4648\n",
            "Epoch 00002: val_accuracy improved from 0.39300 to 0.46300, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 1.4507 - accuracy: 0.4648 - val_loss: 1.5547 - val_accuracy: 0.4630\n",
            "Epoch 3/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3097 - accuracy: 0.5259\n",
            "Epoch 00003: val_accuracy improved from 0.46300 to 0.50360, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 1.3097 - accuracy: 0.5259 - val_loss: 1.4587 - val_accuracy: 0.5036\n",
            "Epoch 4/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2128 - accuracy: 0.5623\n",
            "Epoch 00004: val_accuracy improved from 0.50360 to 0.57770, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 1.2128 - accuracy: 0.5623 - val_loss: 1.2735 - val_accuracy: 0.5777\n",
            "Epoch 5/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1434 - accuracy: 0.5911\n",
            "Epoch 00005: val_accuracy improved from 0.57770 to 0.58450, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 1.1434 - accuracy: 0.5911 - val_loss: 1.2057 - val_accuracy: 0.5845\n",
            "Epoch 6/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0712 - accuracy: 0.6198\n",
            "Epoch 00006: val_accuracy improved from 0.58450 to 0.60990, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 1.0712 - accuracy: 0.6198 - val_loss: 1.1694 - val_accuracy: 0.6099\n",
            "Epoch 7/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0281 - accuracy: 0.6360\n",
            "Epoch 00007: val_accuracy did not improve from 0.60990\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 1.0281 - accuracy: 0.6360 - val_loss: 1.3655 - val_accuracy: 0.5705\n",
            "Epoch 8/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9834 - accuracy: 0.6492\n",
            "Epoch 00008: val_accuracy did not improve from 0.60990\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.9834 - accuracy: 0.6492 - val_loss: 1.3505 - val_accuracy: 0.5611\n",
            "Epoch 9/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9516 - accuracy: 0.6623\n",
            "Epoch 00009: val_accuracy improved from 0.60990 to 0.67270, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.9516 - accuracy: 0.6623 - val_loss: 0.9450 - val_accuracy: 0.6727\n",
            "Epoch 10/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.6747\n",
            "Epoch 00010: val_accuracy did not improve from 0.67270\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.9174 - accuracy: 0.6747 - val_loss: 1.0983 - val_accuracy: 0.6375\n",
            "Epoch 11/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8937 - accuracy: 0.6844\n",
            "Epoch 00011: val_accuracy improved from 0.67270 to 0.68550, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.8937 - accuracy: 0.6844 - val_loss: 0.8986 - val_accuracy: 0.6855\n",
            "Epoch 12/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8688 - accuracy: 0.6935\n",
            "Epoch 00012: val_accuracy did not improve from 0.68550\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.8688 - accuracy: 0.6935 - val_loss: 1.0878 - val_accuracy: 0.6434\n",
            "Epoch 13/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8494 - accuracy: 0.6990\n",
            "Epoch 00013: val_accuracy did not improve from 0.68550\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.8494 - accuracy: 0.6990 - val_loss: 1.2085 - val_accuracy: 0.6201\n",
            "Epoch 14/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8325 - accuracy: 0.7074\n",
            "Epoch 00014: val_accuracy improved from 0.68550 to 0.70740, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.8325 - accuracy: 0.7074 - val_loss: 0.8635 - val_accuracy: 0.7074\n",
            "Epoch 15/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8114 - accuracy: 0.7165\n",
            "Epoch 00015: val_accuracy did not improve from 0.70740\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.8114 - accuracy: 0.7165 - val_loss: 1.0147 - val_accuracy: 0.6557\n",
            "Epoch 16/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7975 - accuracy: 0.7223\n",
            "Epoch 00016: val_accuracy did not improve from 0.70740\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.7975 - accuracy: 0.7223 - val_loss: 1.0108 - val_accuracy: 0.6737\n",
            "Epoch 17/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7831 - accuracy: 0.7253\n",
            "Epoch 00017: val_accuracy did not improve from 0.70740\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.7831 - accuracy: 0.7253 - val_loss: 0.9450 - val_accuracy: 0.6799\n",
            "Epoch 18/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7762 - accuracy: 0.7289\n",
            "Epoch 00018: val_accuracy did not improve from 0.70740\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.7762 - accuracy: 0.7289 - val_loss: 0.9348 - val_accuracy: 0.6864\n",
            "Epoch 19/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7558 - accuracy: 0.7342\n",
            "Epoch 00019: val_accuracy improved from 0.70740 to 0.71010, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.7558 - accuracy: 0.7342 - val_loss: 0.9070 - val_accuracy: 0.7101\n",
            "Epoch 20/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7450 - accuracy: 0.7411\n",
            "Epoch 00020: val_accuracy improved from 0.71010 to 0.73860, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.7450 - accuracy: 0.7411 - val_loss: 0.7730 - val_accuracy: 0.7386\n",
            "Epoch 21/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7367 - accuracy: 0.7417\n",
            "Epoch 00021: val_accuracy did not improve from 0.73860\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.7367 - accuracy: 0.7417 - val_loss: 0.8017 - val_accuracy: 0.7304\n",
            "Epoch 22/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7238 - accuracy: 0.7476\n",
            "Epoch 00022: val_accuracy did not improve from 0.73860\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.7238 - accuracy: 0.7476 - val_loss: 0.9311 - val_accuracy: 0.6916\n",
            "Epoch 23/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7121 - accuracy: 0.7505\n",
            "Epoch 00023: val_accuracy did not improve from 0.73860\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.7121 - accuracy: 0.7505 - val_loss: 1.0110 - val_accuracy: 0.6750\n",
            "Epoch 24/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7026 - accuracy: 0.7547\n",
            "Epoch 00024: val_accuracy improved from 0.73860 to 0.75750, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.7026 - accuracy: 0.7547 - val_loss: 0.7006 - val_accuracy: 0.7575\n",
            "Epoch 25/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6937 - accuracy: 0.7577\n",
            "Epoch 00025: val_accuracy improved from 0.75750 to 0.76050, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 121s 154ms/step - loss: 0.6937 - accuracy: 0.7577 - val_loss: 0.7010 - val_accuracy: 0.7605\n",
            "Epoch 26/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.7614\n",
            "Epoch 00026: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 121s 154ms/step - loss: 0.6853 - accuracy: 0.7614 - val_loss: 0.9216 - val_accuracy: 0.7025\n",
            "Epoch 27/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.7607\n",
            "Epoch 00027: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.6789 - accuracy: 0.7607 - val_loss: 0.7221 - val_accuracy: 0.7533\n",
            "Epoch 28/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6712 - accuracy: 0.7656\n",
            "Epoch 00028: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.6712 - accuracy: 0.7656 - val_loss: 0.9455 - val_accuracy: 0.6967\n",
            "Epoch 29/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6706 - accuracy: 0.7653\n",
            "Epoch 00029: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.6706 - accuracy: 0.7653 - val_loss: 0.9505 - val_accuracy: 0.7018\n",
            "Epoch 30/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.7695\n",
            "Epoch 00030: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.6571 - accuracy: 0.7695 - val_loss: 1.1000 - val_accuracy: 0.6762\n",
            "Epoch 31/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.7732\n",
            "Epoch 00031: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.6565 - accuracy: 0.7732 - val_loss: 0.8448 - val_accuracy: 0.7279\n",
            "Epoch 32/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6531 - accuracy: 0.7718\n",
            "Epoch 00032: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 120s 153ms/step - loss: 0.6531 - accuracy: 0.7718 - val_loss: 0.7974 - val_accuracy: 0.7329\n",
            "Epoch 33/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.7760\n",
            "Epoch 00033: val_accuracy did not improve from 0.76050\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.6390 - accuracy: 0.7760 - val_loss: 0.7383 - val_accuracy: 0.7508\n",
            "Epoch 34/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6350 - accuracy: 0.7786\n",
            "Epoch 00034: val_accuracy improved from 0.76050 to 0.78300, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6350 - accuracy: 0.7786 - val_loss: 0.6438 - val_accuracy: 0.7830\n",
            "Epoch 35/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6247 - accuracy: 0.7838\n",
            "Epoch 00035: val_accuracy did not improve from 0.78300\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6247 - accuracy: 0.7838 - val_loss: 0.6933 - val_accuracy: 0.7659\n",
            "Epoch 36/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7811\n",
            "Epoch 00036: val_accuracy did not improve from 0.78300\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6293 - accuracy: 0.7811 - val_loss: 0.6620 - val_accuracy: 0.7737\n",
            "Epoch 37/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6242 - accuracy: 0.7838\n",
            "Epoch 00037: val_accuracy did not improve from 0.78300\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6242 - accuracy: 0.7838 - val_loss: 0.6896 - val_accuracy: 0.7688\n",
            "Epoch 38/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.7849\n",
            "Epoch 00038: val_accuracy improved from 0.78300 to 0.78740, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6141 - accuracy: 0.7849 - val_loss: 0.6309 - val_accuracy: 0.7874\n",
            "Epoch 39/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.7850\n",
            "Epoch 00039: val_accuracy improved from 0.78740 to 0.79750, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6140 - accuracy: 0.7850 - val_loss: 0.6160 - val_accuracy: 0.7975\n",
            "Epoch 40/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6030 - accuracy: 0.7910\n",
            "Epoch 00040: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6030 - accuracy: 0.7910 - val_loss: 0.7697 - val_accuracy: 0.7514\n",
            "Epoch 41/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.7906\n",
            "Epoch 00041: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6012 - accuracy: 0.7906 - val_loss: 0.6306 - val_accuracy: 0.7867\n",
            "Epoch 42/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.7912\n",
            "Epoch 00042: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5954 - accuracy: 0.7912 - val_loss: 0.7234 - val_accuracy: 0.7623\n",
            "Epoch 43/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.7924\n",
            "Epoch 00043: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5905 - accuracy: 0.7924 - val_loss: 0.6394 - val_accuracy: 0.7882\n",
            "Epoch 44/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.7942\n",
            "Epoch 00044: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.5897 - accuracy: 0.7942 - val_loss: 0.6723 - val_accuracy: 0.7705\n",
            "Epoch 45/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.7959\n",
            "Epoch 00045: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5840 - accuracy: 0.7959 - val_loss: 0.6378 - val_accuracy: 0.7822\n",
            "Epoch 46/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.7961\n",
            "Epoch 00046: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5808 - accuracy: 0.7961 - val_loss: 0.5950 - val_accuracy: 0.7970\n",
            "Epoch 47/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.7986\n",
            "Epoch 00047: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5750 - accuracy: 0.7986 - val_loss: 0.6920 - val_accuracy: 0.7752\n",
            "Epoch 48/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.7991\n",
            "Epoch 00048: val_accuracy did not improve from 0.79750\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.5744 - accuracy: 0.7991 - val_loss: 0.7180 - val_accuracy: 0.7575\n",
            "Epoch 49/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.7980\n",
            "Epoch 00049: val_accuracy improved from 0.79750 to 0.79800, saving model to weights.best.hdf5\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5751 - accuracy: 0.7980 - val_loss: 0.6035 - val_accuracy: 0.7980\n",
            "Epoch 50/50\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.8019\n",
            "Epoch 00050: val_accuracy did not improve from 0.79800\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.5691 - accuracy: 0.8019 - val_loss: 0.8045 - val_accuracy: 0.7401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gZNFJv3mo4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3368991e-1dc7-4128-9b75-e11597431d13"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.grid()\n",
        "plt.title('model_accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UE3lF6EH1r_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b779facb-776f-42d7-a0c8-312eabf397ee"
      },
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"DNST_model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZcWydmIVhZGr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f4779882-e91c-4008-a3f8-779d05ee193f"
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 8s 26ms/step - loss: 0.8045 - accuracy: 0.7401\n",
            "Test loss: 0.8045022487640381\n",
            "Test accuracy: 0.7401000261306763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYl_ZPrt5hjc",
        "colab_type": "text"
      },
      "source": [
        "Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Ucfmjixibn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(width_shift_range=0.3, height_shift_range=0.3, horizontal_flip=True,rotation_range=15)\n",
        "datagen.fit(X_train)\n",
        "it_train = datagen.flow(X_train, y_train, batch_size=64)\n",
        "steps = int(X_train.shape[0] / 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbxqTxG-1vHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "l = 40\n",
        "num_filter = 24\n",
        "compression = 0.5\n",
        "compression_1 = 2\n",
        "compression_2 = 4\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAZjqihY70hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    global compression_1\n",
        "    global compression_2\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        #if dropout_rate>0:\n",
        "            #Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Block\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    #if dropout_rate>0:\n",
        "         #Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    #flat = layers.Flatten()(AvgPooling)\n",
        "    First = layers.Conv2D(int(10), (2,2), use_bias=False ,padding='same')(AvgPooling)\n",
        "    Maxpool = layers.GlobalMaxPooling2D()(First)\n",
        "    output = layers.Activation(\"softmax\")(Maxpool)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPIfJTY5lHq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_filter = 24\n",
        "dropout_rate = 0.2\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRN7z_BF7i-W",
        "colab_type": "code",
        "outputId": "6ae3b3e5-b613-4b2a-e757-b675bed8afd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8765
        }
      },
      "source": [
        "model_2 = Model(inputs=[input], outputs=[output])\n",
        "model_2.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 24)   648         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 24)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 24)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 12)   2592        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 36)   0           conv2d[0][0]                     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 36)   144         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   3888        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 48)   0           concatenate[0][0]                \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   5184        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 60)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 60)   240         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 60)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   6480        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 72)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 72)   288         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 72)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   7776        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 84)   0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 84)   336         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 84)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   9072        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 96)   0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 96)   384         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 96)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   10368       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 108)  0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 108)  432         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 108)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   11664       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 120)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 120)  480         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 120)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   12960       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 132)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 132)  528         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 132)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   14256       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 144)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 144)  576         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 144)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   15552       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 156)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 156)  624         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 156)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   16848       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 168)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 168)  672         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 168)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   2016        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 12)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 12)   48          average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 12)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 12)   1296        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 24)   0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 24)   96          concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 24)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 12)   2592        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 36)   0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 36)   144         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 36)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 12)   3888        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 48)   0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 48)   192         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 12)   5184        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 60)   0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 60)   240         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 60)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 12)   6480        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 72)   0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 72)   288         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 72)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 12)   7776        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 84)   0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 84)   336         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 84)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 12)   9072        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 96)   0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 96)   384         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 96)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   10368       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 108)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 108)  432         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 108)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   11664       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 120)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 120)  480         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 120)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   12960       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 132)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 132)  528         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 132)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   14256       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 144)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 144)  576         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 144)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   15552       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 156)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 156)  624         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 156)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 12)   1872        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 12)     0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 12)     48          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 12)     0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 12)     1296        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 8, 8, 24)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 24)     96          concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 24)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 12)     2592        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 36)     0           concatenate_24[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 36)     144         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 36)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 12)     3888        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 48)     0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 48)     192         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 48)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 12)     5184        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 60)     0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 60)     240         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 60)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 12)     6480        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 72)     0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 72)     288         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 72)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 12)     7776        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 84)     0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 84)     336         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 84)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 12)     9072        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 96)     0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 96)     384         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 96)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 12)     10368       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 108)    0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 108)    432         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 108)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 12)     11664       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 120)    0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 120)    480         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 120)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 12)     12960       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 132)    0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 132)    528         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 132)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 12)     14256       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 144)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 144)    576         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 144)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 12)     15552       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 156)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 156)    624         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 156)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 12)     1872        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 12)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 4, 12)     48          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 4, 4, 12)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 4, 4, 12)     1296        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 4, 4, 24)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 24)     96          concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 24)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 12)     2592        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 36)     0           concatenate_36[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 36)     144         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 36)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 12)     3888        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 48)     0           concatenate_37[0][0]             \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 48)     192         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 48)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 12)     5184        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 60)     0           concatenate_38[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 60)     240         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 60)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 12)     6480        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 72)     0           concatenate_39[0][0]             \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 72)     288         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 72)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 12)     7776        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 84)     0           concatenate_40[0][0]             \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 84)     336         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 84)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 12)     9072        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 96)     0           concatenate_41[0][0]             \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 96)     384         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 96)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 12)     10368       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 108)    0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 108)    432         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 108)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 12)     11664       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 120)    0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 120)    480         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 120)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 12)     12960       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 132)    0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 132)    528         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 132)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 12)     14256       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 144)    0           concatenate_45[0][0]             \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 144)    576         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 144)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 12)     15552       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 156)    0           concatenate_46[0][0]             \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 156)    624         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 156)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 156)    0           activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 2, 2, 10)     6240        average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 10)           0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 10)           0           global_max_pooling2d[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 450,648\n",
            "Trainable params: 441,600\n",
            "Non-trainable params: 9,048\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWiSFdDt_sIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "84bba9d0-821f-466f-e723-24de4d96942b"
      },
      "source": [
        "#https://www.tensorflow.org/tensorboard/scalars_and_keras\n",
        "filepath=\"weights_3.best.hdf5\"\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "callback_2 = tf.keras.callbacks.ModelCheckpoint(filepath=filepath ,\n",
        "                                save_weights_only=True,\n",
        "                                monitor=\"val_accuracy\",\n",
        "                                mode=\"max\",\n",
        "                                save_best_only=True,\n",
        "                                verbose=1)\n",
        "tensorboard_2 = TensorBoard(log_dir='graph_one', batch_size=64,update_freq='epoch')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v2.py:92: UserWarning: The TensorBoard callback `batch_size` argument (for histogram computation) is deprecated with TensorFlow 2.0. It will be ignored.\n",
            "  warnings.warn('The TensorBoard callback `batch_size` argument '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZe9-J77mFLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prpluenp_4_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5510
        },
        "outputId": "cd72872a-395e-472c-b8fd-4ace18626424"
      },
      "source": [
        "history = model_2.fit_generator(img_train,\n",
        "                    steps_per_epoch=steps,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=callback_2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-370768f647ea>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.7078 - accuracy: 0.3614\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.48190, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 64s 82ms/step - loss: 1.7078 - accuracy: 0.3614 - val_loss: 1.4209 - val_accuracy: 0.4819\n",
            "Epoch 2/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3442 - accuracy: 0.5120\n",
            "Epoch 00002: val_accuracy did not improve from 0.48190\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 1.3442 - accuracy: 0.5120 - val_loss: 1.9366 - val_accuracy: 0.4339\n",
            "Epoch 3/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1557 - accuracy: 0.5852\n",
            "Epoch 00003: val_accuracy improved from 0.48190 to 0.62060, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 1.1557 - accuracy: 0.5852 - val_loss: 1.0709 - val_accuracy: 0.6206\n",
            "Epoch 4/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0311 - accuracy: 0.6308\n",
            "Epoch 00004: val_accuracy did not improve from 0.62060\n",
            "781/781 [==============================] - 60s 76ms/step - loss: 1.0311 - accuracy: 0.6308 - val_loss: 1.1683 - val_accuracy: 0.6066\n",
            "Epoch 5/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9492 - accuracy: 0.6617\n",
            "Epoch 00005: val_accuracy improved from 0.62060 to 0.66110, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.9492 - accuracy: 0.6617 - val_loss: 1.0082 - val_accuracy: 0.6611\n",
            "Epoch 6/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8879 - accuracy: 0.6873\n",
            "Epoch 00006: val_accuracy did not improve from 0.66110\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.8879 - accuracy: 0.6873 - val_loss: 1.4715 - val_accuracy: 0.5564\n",
            "Epoch 7/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.7101\n",
            "Epoch 00007: val_accuracy improved from 0.66110 to 0.68110, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.8286 - accuracy: 0.7101 - val_loss: 0.9707 - val_accuracy: 0.6811\n",
            "Epoch 8/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7874 - accuracy: 0.7234\n",
            "Epoch 00008: val_accuracy improved from 0.68110 to 0.74310, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.7874 - accuracy: 0.7234 - val_loss: 0.7654 - val_accuracy: 0.7431\n",
            "Epoch 9/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7508 - accuracy: 0.7364\n",
            "Epoch 00009: val_accuracy did not improve from 0.74310\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.7508 - accuracy: 0.7364 - val_loss: 0.9285 - val_accuracy: 0.6926\n",
            "Epoch 10/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7149 - accuracy: 0.7495\n",
            "Epoch 00010: val_accuracy did not improve from 0.74310\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.7149 - accuracy: 0.7495 - val_loss: 0.7695 - val_accuracy: 0.7337\n",
            "Epoch 11/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.7604\n",
            "Epoch 00011: val_accuracy did not improve from 0.74310\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.6894 - accuracy: 0.7604 - val_loss: 0.8247 - val_accuracy: 0.7225\n",
            "Epoch 12/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6611 - accuracy: 0.7710\n",
            "Epoch 00012: val_accuracy improved from 0.74310 to 0.78210, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.6611 - accuracy: 0.7710 - val_loss: 0.6519 - val_accuracy: 0.7821\n",
            "Epoch 13/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.7775\n",
            "Epoch 00013: val_accuracy did not improve from 0.78210\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.6366 - accuracy: 0.7775 - val_loss: 0.7034 - val_accuracy: 0.7638\n",
            "Epoch 14/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6210 - accuracy: 0.7850\n",
            "Epoch 00014: val_accuracy did not improve from 0.78210\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.6210 - accuracy: 0.7850 - val_loss: 0.7885 - val_accuracy: 0.7410\n",
            "Epoch 15/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6018 - accuracy: 0.7919\n",
            "Epoch 00015: val_accuracy did not improve from 0.78210\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.6018 - accuracy: 0.7919 - val_loss: 0.6770 - val_accuracy: 0.7799\n",
            "Epoch 16/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.7977\n",
            "Epoch 00016: val_accuracy did not improve from 0.78210\n",
            "781/781 [==============================] - 59s 75ms/step - loss: 0.5843 - accuracy: 0.7977 - val_loss: 0.7066 - val_accuracy: 0.7675\n",
            "Epoch 17/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8028\n",
            "Epoch 00017: val_accuracy did not improve from 0.78210\n",
            "781/781 [==============================] - 59s 75ms/step - loss: 0.5673 - accuracy: 0.8028 - val_loss: 0.7461 - val_accuracy: 0.7654\n",
            "Epoch 18/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.8087\n",
            "Epoch 00018: val_accuracy improved from 0.78210 to 0.78360, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 60s 76ms/step - loss: 0.5511 - accuracy: 0.8087 - val_loss: 0.6725 - val_accuracy: 0.7836\n",
            "Epoch 19/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5400 - accuracy: 0.8120\n",
            "Epoch 00019: val_accuracy did not improve from 0.78360\n",
            "781/781 [==============================] - 59s 75ms/step - loss: 0.5400 - accuracy: 0.8120 - val_loss: 0.6973 - val_accuracy: 0.7754\n",
            "Epoch 20/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.8178\n",
            "Epoch 00020: val_accuracy improved from 0.78360 to 0.82710, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 59s 75ms/step - loss: 0.5283 - accuracy: 0.8178 - val_loss: 0.5188 - val_accuracy: 0.8271\n",
            "Epoch 21/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5183 - accuracy: 0.8212\n",
            "Epoch 00021: val_accuracy did not improve from 0.82710\n",
            "781/781 [==============================] - 58s 75ms/step - loss: 0.5183 - accuracy: 0.8212 - val_loss: 0.5524 - val_accuracy: 0.8158\n",
            "Epoch 22/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.8248\n",
            "Epoch 00022: val_accuracy did not improve from 0.82710\n",
            "781/781 [==============================] - 59s 75ms/step - loss: 0.5082 - accuracy: 0.8248 - val_loss: 0.5533 - val_accuracy: 0.8178\n",
            "Epoch 23/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.8280\n",
            "Epoch 00023: val_accuracy did not improve from 0.82710\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.4998 - accuracy: 0.8280 - val_loss: 0.6017 - val_accuracy: 0.7996\n",
            "Epoch 24/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4863 - accuracy: 0.8311\n",
            "Epoch 00024: val_accuracy did not improve from 0.82710\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.4863 - accuracy: 0.8311 - val_loss: 0.5809 - val_accuracy: 0.8145\n",
            "Epoch 25/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.8350\n",
            "Epoch 00025: val_accuracy improved from 0.82710 to 0.83970, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 59s 75ms/step - loss: 0.4740 - accuracy: 0.8350 - val_loss: 0.4766 - val_accuracy: 0.8397\n",
            "Epoch 26/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.8368\n",
            "Epoch 00026: val_accuracy did not improve from 0.83970\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.4676 - accuracy: 0.8368 - val_loss: 0.5126 - val_accuracy: 0.8313\n",
            "Epoch 27/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.8422\n",
            "Epoch 00027: val_accuracy did not improve from 0.83970\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.4541 - accuracy: 0.8422 - val_loss: 0.5352 - val_accuracy: 0.8225\n",
            "Epoch 28/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.8450\n",
            "Epoch 00028: val_accuracy did not improve from 0.83970\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.4499 - accuracy: 0.8450 - val_loss: 0.5655 - val_accuracy: 0.8159\n",
            "Epoch 29/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8473\n",
            "Epoch 00029: val_accuracy did not improve from 0.83970\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.4430 - accuracy: 0.8473 - val_loss: 0.5665 - val_accuracy: 0.8187\n",
            "Epoch 30/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4370 - accuracy: 0.8478\n",
            "Epoch 00030: val_accuracy did not improve from 0.83970\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.4370 - accuracy: 0.8478 - val_loss: 0.4931 - val_accuracy: 0.8393\n",
            "Epoch 31/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8511\n",
            "Epoch 00031: val_accuracy improved from 0.83970 to 0.84450, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.4283 - accuracy: 0.8511 - val_loss: 0.4590 - val_accuracy: 0.8445\n",
            "Epoch 32/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4161 - accuracy: 0.8543\n",
            "Epoch 00032: val_accuracy did not improve from 0.84450\n",
            "781/781 [==============================] - 58s 75ms/step - loss: 0.4161 - accuracy: 0.8543 - val_loss: 0.5467 - val_accuracy: 0.8242\n",
            "Epoch 33/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4161 - accuracy: 0.8577\n",
            "Epoch 00033: val_accuracy did not improve from 0.84450\n",
            "781/781 [==============================] - 65s 83ms/step - loss: 0.4161 - accuracy: 0.8577 - val_loss: 0.6014 - val_accuracy: 0.8107\n",
            "Epoch 34/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.8566\n",
            "Epoch 00034: val_accuracy improved from 0.84450 to 0.85680, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.4128 - accuracy: 0.8566 - val_loss: 0.4464 - val_accuracy: 0.8568\n",
            "Epoch 35/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8600\n",
            "Epoch 00035: val_accuracy did not improve from 0.85680\n",
            "781/781 [==============================] - 61s 79ms/step - loss: 0.4032 - accuracy: 0.8600 - val_loss: 0.4822 - val_accuracy: 0.8446\n",
            "Epoch 36/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.8606\n",
            "Epoch 00036: val_accuracy improved from 0.85680 to 0.85860, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 61s 79ms/step - loss: 0.4020 - accuracy: 0.8606 - val_loss: 0.4317 - val_accuracy: 0.8586\n",
            "Epoch 37/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3871 - accuracy: 0.8650\n",
            "Epoch 00037: val_accuracy did not improve from 0.85860\n",
            "781/781 [==============================] - 62s 80ms/step - loss: 0.3871 - accuracy: 0.8650 - val_loss: 0.5045 - val_accuracy: 0.8356\n",
            "Epoch 38/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.8647\n",
            "Epoch 00038: val_accuracy did not improve from 0.85860\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3900 - accuracy: 0.8647 - val_loss: 0.5285 - val_accuracy: 0.8306\n",
            "Epoch 39/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.8664\n",
            "Epoch 00039: val_accuracy did not improve from 0.85860\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3832 - accuracy: 0.8664 - val_loss: 0.4383 - val_accuracy: 0.8519\n",
            "Epoch 40/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.8674\n",
            "Epoch 00040: val_accuracy improved from 0.85860 to 0.86980, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3812 - accuracy: 0.8674 - val_loss: 0.3963 - val_accuracy: 0.8698\n",
            "Epoch 41/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.8700\n",
            "Epoch 00041: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 61s 79ms/step - loss: 0.3710 - accuracy: 0.8700 - val_loss: 0.5843 - val_accuracy: 0.8210\n",
            "Epoch 42/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3699 - accuracy: 0.8711\n",
            "Epoch 00042: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3699 - accuracy: 0.8711 - val_loss: 0.3984 - val_accuracy: 0.8679\n",
            "Epoch 43/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8736\n",
            "Epoch 00043: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.3648 - accuracy: 0.8736 - val_loss: 0.6063 - val_accuracy: 0.8101\n",
            "Epoch 44/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8723\n",
            "Epoch 00044: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3646 - accuracy: 0.8723 - val_loss: 0.4679 - val_accuracy: 0.8483\n",
            "Epoch 45/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3592 - accuracy: 0.8761\n",
            "Epoch 00045: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3592 - accuracy: 0.8761 - val_loss: 0.4600 - val_accuracy: 0.8535\n",
            "Epoch 46/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.8762\n",
            "Epoch 00046: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 61s 79ms/step - loss: 0.3547 - accuracy: 0.8762 - val_loss: 0.4551 - val_accuracy: 0.8592\n",
            "Epoch 47/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8789\n",
            "Epoch 00047: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.3490 - accuracy: 0.8789 - val_loss: 0.4614 - val_accuracy: 0.8531\n",
            "Epoch 48/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.8793\n",
            "Epoch 00048: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.3451 - accuracy: 0.8793 - val_loss: 0.4438 - val_accuracy: 0.8561\n",
            "Epoch 49/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8803\n",
            "Epoch 00049: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.3427 - accuracy: 0.8803 - val_loss: 0.4451 - val_accuracy: 0.8569\n",
            "Epoch 50/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.8800\n",
            "Epoch 00050: val_accuracy did not improve from 0.86980\n",
            "781/781 [==============================] - 63s 80ms/step - loss: 0.3389 - accuracy: 0.8800 - val_loss: 0.4137 - val_accuracy: 0.8642\n",
            "Epoch 51/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8820\n",
            "Epoch 00051: val_accuracy improved from 0.86980 to 0.87440, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 61s 79ms/step - loss: 0.3379 - accuracy: 0.8820 - val_loss: 0.3945 - val_accuracy: 0.8744\n",
            "Epoch 52/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.8853\n",
            "Epoch 00052: val_accuracy improved from 0.87440 to 0.87600, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3324 - accuracy: 0.8853 - val_loss: 0.3896 - val_accuracy: 0.8760\n",
            "Epoch 53/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3265 - accuracy: 0.8859\n",
            "Epoch 00053: val_accuracy did not improve from 0.87600\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3265 - accuracy: 0.8859 - val_loss: 0.4545 - val_accuracy: 0.8594\n",
            "Epoch 54/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8861\n",
            "Epoch 00054: val_accuracy did not improve from 0.87600\n",
            "781/781 [==============================] - 62s 80ms/step - loss: 0.3278 - accuracy: 0.8861 - val_loss: 0.4560 - val_accuracy: 0.8547\n",
            "Epoch 55/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.8866\n",
            "Epoch 00055: val_accuracy did not improve from 0.87600\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3245 - accuracy: 0.8866 - val_loss: 0.4278 - val_accuracy: 0.8599\n",
            "Epoch 56/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8892\n",
            "Epoch 00056: val_accuracy did not improve from 0.87600\n",
            "781/781 [==============================] - 61s 79ms/step - loss: 0.3209 - accuracy: 0.8892 - val_loss: 0.4604 - val_accuracy: 0.8559\n",
            "Epoch 57/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8896\n",
            "Epoch 00057: val_accuracy did not improve from 0.87600\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.3170 - accuracy: 0.8896 - val_loss: 0.4487 - val_accuracy: 0.8576\n",
            "Epoch 58/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8894\n",
            "Epoch 00058: val_accuracy improved from 0.87600 to 0.88360, saving model to weights_3.best.hdf5\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.3176 - accuracy: 0.8894 - val_loss: 0.3539 - val_accuracy: 0.8836\n",
            "Epoch 59/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8917\n",
            "Epoch 00059: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 62s 79ms/step - loss: 0.3107 - accuracy: 0.8917 - val_loss: 0.4208 - val_accuracy: 0.8697\n",
            "Epoch 60/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8923\n",
            "Epoch 00060: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.3079 - accuracy: 0.8923 - val_loss: 0.4984 - val_accuracy: 0.8484\n",
            "Epoch 61/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8934\n",
            "Epoch 00061: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.3072 - accuracy: 0.8934 - val_loss: 0.4834 - val_accuracy: 0.8488\n",
            "Epoch 62/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.8913\n",
            "Epoch 00062: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.3088 - accuracy: 0.8913 - val_loss: 0.4529 - val_accuracy: 0.8569\n",
            "Epoch 63/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8962\n",
            "Epoch 00063: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2969 - accuracy: 0.8962 - val_loss: 0.5774 - val_accuracy: 0.8334\n",
            "Epoch 64/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.8939\n",
            "Epoch 00064: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2995 - accuracy: 0.8939 - val_loss: 0.4959 - val_accuracy: 0.8550\n",
            "Epoch 65/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.8973\n",
            "Epoch 00065: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2955 - accuracy: 0.8973 - val_loss: 0.3864 - val_accuracy: 0.8777\n",
            "Epoch 66/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.8999\n",
            "Epoch 00066: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2900 - accuracy: 0.8999 - val_loss: 0.4035 - val_accuracy: 0.8730\n",
            "Epoch 67/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9000\n",
            "Epoch 00067: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2881 - accuracy: 0.9000 - val_loss: 0.4484 - val_accuracy: 0.8597\n",
            "Epoch 68/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.8982\n",
            "Epoch 00068: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.2893 - accuracy: 0.8982 - val_loss: 0.4436 - val_accuracy: 0.8641\n",
            "Epoch 69/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.8992\n",
            "Epoch 00069: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 58s 74ms/step - loss: 0.2873 - accuracy: 0.8992 - val_loss: 0.5121 - val_accuracy: 0.8483\n",
            "Epoch 70/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9015\n",
            "Epoch 00070: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.2857 - accuracy: 0.9015 - val_loss: 0.4016 - val_accuracy: 0.8731\n",
            "Epoch 71/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.8993\n",
            "Epoch 00071: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 58s 75ms/step - loss: 0.2837 - accuracy: 0.8993 - val_loss: 0.4269 - val_accuracy: 0.8659\n",
            "Epoch 72/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.9004\n",
            "Epoch 00072: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2837 - accuracy: 0.9004 - val_loss: 0.5268 - val_accuracy: 0.8478\n",
            "Epoch 73/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.9007\n",
            "Epoch 00073: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2806 - accuracy: 0.9007 - val_loss: 0.4009 - val_accuracy: 0.8744\n",
            "Epoch 74/100\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.9050\n",
            "Epoch 00074: val_accuracy did not improve from 0.88360\n",
            "781/781 [==============================] - 61s 78ms/step - loss: 0.2735 - accuracy: 0.9050 - val_loss: 0.4130 - val_accuracy: 0.8759\n",
            "Epoch 75/100\n",
            "427/781 [===============>..............] - ETA: 25s - loss: 0.2755 - accuracy: 0.9044Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfEqEd0yOcNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "697dd9b5-46fe-49c6-8c47-fa451fc219a7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.grid()\n",
        "plt.title('model_accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bql-zcx8BTFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.load_weights(\"weights_3.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d0c5CaXBTDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.compile(\n",
        "loss='categorical_crossentropy',\n",
        "optimizer=Adam(),\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Jz-BbnA9et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "18dd9051-adc0-4e34-d9d9-22c8d64314e1"
      },
      "source": [
        "# Test the model\n",
        "score = model_2.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 11ms/step - loss: 0.3292 - accuracy: 0.8983\n",
            "Test loss: 0.32916033267974854\n",
            "Test accuracy: 0.8982999920845032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zArvUhYk-pwx",
        "colab_type": "text"
      },
      "source": [
        "Training for 20 more epochs for Better Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TZsarcy9AAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1165
        },
        "outputId": "94d32189-ef9c-4dcf-ca4e-06453b0266d9"
      },
      "source": [
        "history = model_2.fit_generator(img_train,\n",
        "                    steps_per_epoch=steps,\n",
        "                    epochs=20,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=callback_2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9312\n",
            "Epoch 00013: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1997 - accuracy: 0.9312 - val_loss: 0.4479 - val_accuracy: 0.8747\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1997 - accuracy: 0.9312 - val_loss: 0.4479 - val_accuracy: 0.8747\n",
            "Epoch 14/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.2804 - accuracy: 0.9062Epoch 14/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9304\n",
            "Epoch 00014: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 76ms/step - loss: 0.1958 - accuracy: 0.9304 - val_loss: 0.3857 - val_accuracy: 0.8926\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 76ms/step - loss: 0.1958 - accuracy: 0.9304 - val_loss: 0.3857 - val_accuracy: 0.8926\n",
            "Epoch 15/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.2132 - accuracy: 0.9219Epoch 15/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9280\n",
            "Epoch 00015: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2012 - accuracy: 0.9280 - val_loss: 0.4102 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.2012 - accuracy: 0.9280 - val_loss: 0.4102 - val_accuracy: 0.8886\n",
            "Epoch 16/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.1446 - accuracy: 0.9688Epoch 16/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9311\n",
            "Epoch 00016: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1980 - accuracy: 0.9311 - val_loss: 0.3945 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1980 - accuracy: 0.9311 - val_loss: 0.3945 - val_accuracy: 0.8855\n",
            "Epoch 17/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.2155 - accuracy: 0.9062Epoch 17/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9298\n",
            "Epoch 00017: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.1963 - accuracy: 0.9298 - val_loss: 0.3696 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.1963 - accuracy: 0.9298 - val_loss: 0.3696 - val_accuracy: 0.8975\n",
            "Epoch 18/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.2822 - accuracy: 0.9219Epoch 18/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9299\n",
            "Epoch 00018: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1992 - accuracy: 0.9299 - val_loss: 0.4651 - val_accuracy: 0.8702\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1992 - accuracy: 0.9299 - val_loss: 0.4651 - val_accuracy: 0.8702\n",
            "Epoch 19/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.3081 - accuracy: 0.8750Epoch 19/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1944 - accuracy: 0.9321\n",
            "Epoch 00019: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1944 - accuracy: 0.9321 - val_loss: 0.3540 - val_accuracy: 0.9005\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 60s 77ms/step - loss: 0.1944 - accuracy: 0.9321 - val_loss: 0.3540 - val_accuracy: 0.9005\n",
            "Epoch 20/20\n",
            "  1/781 [..............................] - ETA: 0s - loss: 0.4130 - accuracy: 0.8906Epoch 20/20\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9313\n",
            "Epoch 00020: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.1958 - accuracy: 0.9313 - val_loss: 0.3840 - val_accuracy: 0.8914\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90280\n",
            "781/781 [==============================] - 59s 76ms/step - loss: 0.1958 - accuracy: 0.9313 - val_loss: 0.3840 - val_accuracy: 0.8914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVIT81P23et6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_2.load_weights(\"weights_3.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TCptmHe-Y0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e7a70334-436f-4131-8c8d-32be27a8d1f9"
      },
      "source": [
        "# Test the model\n",
        "score = model_2.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 4s 11ms/step - loss: 0.3397 - accuracy: 0.9028\n",
            "Test loss: 0.3396528363227844\n",
            "Test accuracy: 0.9028000235557556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYLijyuy-hA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "a0bfd392-08ed-42ba-e511-d96496aec872"
      },
      "source": [
        "# Please compare all your models using Prettytable library\n",
        "from prettytable import PrettyTable\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Model\", \"kernel\", \"filters\", \"Train Accuracy(%)\", \"Test accuracy(%)\"]\n",
        "x.add_row([\"Model 1\", \"3x3\", 12, 80.19, 74.01]),\n",
        "x.add_row([\"Model 2\", \"3x3\", 24, 93.13,90.28]),\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------+---------+-------------------+------------------+\n",
            "|  Model  | kernel | filters | Train Accuracy(%) | Test accuracy(%) |\n",
            "+---------+--------+---------+-------------------+------------------+\n",
            "| Model 1 |  3x3   |    12   |       80.19       |      74.01       |\n",
            "| Model 2 |  3x3   |    24   |       93.13       |      90.28       |\n",
            "+---------+--------+---------+-------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV88Mj6gBLbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}